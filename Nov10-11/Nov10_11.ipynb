{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMjyyxqPKZ1PBvOC1Ri9Aku",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DCI-alxogm/ml2023-Diego1733/blob/main/Nov10-11/Nov10_11.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Propagación hacia adelante:\n",
        "\n",
        "Lo que queremos es querer construir la red neuronal, cuántas capas y neuronas (Basicamente, cuántos pesos).\n",
        "\n",
        "### Propagación hacia atrás:\n",
        "\n",
        "Cómo optimizo esos pesos. Cuando se habla de redes neuronales, se trata de pasar por todas las neuronas, optimizando todos los pesos.\n",
        "\n",
        "\n",
        "····································································································································································································································\n",
        "\n",
        "g(X) = activación de neurona\n",
        "\n",
        "LO QUE DEFINE A UNA NEURONA SON SUS PESOS Y SUS BAYES (B)\n",
        "\n",
        "Hasta ahora sólo hemos visto una sola neurona, con pesos de la dimensión de la neurona. Construiremos un modelo un poco más avanzado. Pensando en un problema \"Predicción de la demanda\", estamos en una empresa de X productos, un equipo tiene un producto nuevo y queremos predecir si el producto tendrá una alta demanda en el mercado.\n",
        "\n",
        "* Analizandolo como humanos: Nuestras variables podrían ser: El producto, el precio, el costo de traslado, marketing, material.\n",
        "La salida sería la \"demanda\", podríamos interpretarlo como la probabilidad de que sea un producto muy solicitado. Podríamos hacer una regresión lineal, o verlo como un problema de PCA y predecir.\n",
        "También podríamos combinarlas y crear nuevas variables:\n",
        "    \n",
        "\n",
        "1.   Accesibilidad: Producto, Precio\n",
        "2.   Conocimiento: Marketing, Producto\n",
        "3.   Percepción de la calidad: Producto, Material\n",
        "\n",
        "\n",
        "\n",
        "* Analizando como red neuronal: Cada una de las variables originales podría verse como una neurona con entradas y da una salida, y estas se pueden conectar a una nueva neurona que da la salida a la demanda. Esto sería una neurona de 3 capas. En las redes neuronales no nos importan las variables, nos importan las relaciones que va encontrando en términos de las variables originales. Internamente la red neuronal la red está haciendo una especia de reducción de variables, la que no son útiles, su costo es 0, pero no es específicamente un PCA. La propia neurona se encarga de predecir y en base a la correlación descarta las variables que no contribuyen. ¿Cómo sé que me está prediciendo cada neurona? Generalmente no sabemos, lo importante es el resultado final (Tengo entendido).\n",
        "\n",
        "En una red neuronal, las neuronas no \"saben\" de antemano qué hacer o qué predecir. Más bien, el proceso de aprendizaje implica ajustar los pesos y sesgos de las conexiones entre neuronas para que la red pueda realizar tareas específicas.\n",
        "\n",
        "Cuando se inicializa una red neuronal, los pesos y sesgos se establecen generalmente de manera aleatoria. Durante el entrenamiento, la red se expone a un conjunto de datos de entrenamiento que contiene ejemplos emparejados de entrada y salida deseada. La red ajusta sus pesos y sesgos utilizando un proceso llamado retropropagación, que se basa en el cálculo del gradiente de la función de pérdida con respecto a los pesos.\n",
        "\n",
        "\n",
        "\n",
        "····································································································································································································································\n",
        "### ¿Cómo funciona una red neuronal?\n",
        "\n",
        "\n",
        "1.  Entrada (Input Layer): Esta capa recibe los datos de entrada. Cada nodo en esta capa representa una característica o variable del conjunto de datos.\n",
        "\n",
        "2.  Pesos y Bias: Cada conexión entre neuronas tiene un peso asociado, que determina la fuerza de la conexión. Además, cada neurona tiene un término de sesgo (bias) que se suma a la entrada ponderada antes de aplicar una función de activación.\n",
        "\n",
        "3.  Función de Activación: Después de calcular la suma ponderada de las entradas, se aplica una función de activación a ese valor. La función de activación introduce no linealidades en la red, permitiendo que la red aprenda patrones más complejos.\n",
        "\n",
        "4.  Capas Ocultas (Hidden Layers): Después de la capa de entrada, hay una o más capas ocultas donde se realizan cálculos más complejos. Cada capa oculta tiene su propio conjunto de pesos y bias.\n",
        "\n",
        "5.  Salida (Output Layer): La última capa produce el resultado final. La cantidad de nodos en esta capa depende del tipo de problema: clasificación binaria, clasificación multiclase, regresión, etc.\n",
        "\n",
        "6.  Función de Pérdida (Loss Function): Se utiliza una función de pérdida para evaluar cuán bien está haciendo la red en comparación con la verdad conocida. La red ajusta sus pesos y bias para minimizar esta pérdida durante el entrenamiento.\n",
        "\n",
        "7.  Optimización: Se utiliza un algoritmo de optimización (como el descenso de gradiente) para ajustar los pesos y los bias de la red de manera que se minimice la función de pérdida.\n",
        "\n",
        "8.  Retropropagación (Backpropagation): Este es el proceso mediante el cual se calcula el gradiente de la función de pérdida con respecto a los pesos de la red. Luego, se utiliza este gradiente para actualizar los pesos mediante el algoritmo de optimización.\n",
        "\n",
        "9.  Entrenamiento: El modelo se entrena mediante la presentación iterativa de ejemplos de entrenamiento. La red ajusta sus pesos y bias durante el proceso de retropropagación para mejorar su rendimiento.\n",
        "\n",
        "\n",
        "····································································································································································································································\n",
        "### Tipos de arquitectura en redes neuronales:\n",
        "\n",
        "1.  Redes Neuronales Feedforward (FNN):\n",
        "\n",
        "Estas son las redes neuronales más simples, donde la información fluye en una sola dirección, desde la capa de entrada hasta la capa de salida. Son eficaces para tareas de clasificación y regresión simples.\n",
        "2.  Redes Neuronales Convolucionales (CNN):\n",
        "\n",
        "Diseñadas especialmente para procesar datos de tipo cuadrícula, como imágenes. Son muy eficientes para tareas de visión por computadora, reconocimiento de objetos y segmentación de imágenes.\n",
        "3.  Redes Neuronales Recurrentes (RNN):\n",
        "\n",
        "Adecuadas para datos secuenciales, como series temporales o texto. Son útiles en tareas como el procesamiento del lenguaje natural (PLN), traducción automática y predicción de series temporales.\n",
        "4.  Redes Neuronales LSTM (Long Short-Term Memory):\n",
        "\n",
        "Una variante de las RNN diseñada para abordar el problema de la desaparición del gradiente. Se utilizan comúnmente en tareas que involucran secuencias a largo plazo, como la generación de texto y la traducción automática.\n",
        "5.  Redes Neuronales Generativas Adversariales (GAN):\n",
        "\n",
        "Utilizadas para la generación de datos nuevos y realistas. Son populares en aplicaciones de generación de imágenes, como la creación de caras de personas que no existen.\n",
        "6.  Redes Neuronales Transformer:\n",
        "\n",
        "Introducidas inicialmente para tareas de procesamiento de lenguaje natural, como la traducción automática. Han demostrado ser eficaces en una variedad de aplicaciones, incluida la visión por computadora y el aprendizaje por refuerzo.\n",
        "7.  Redes Neuronales Siamesas:\n",
        "\n",
        "Diseñadas para comparar similitudes entre dos entradas. Son útiles en problemas como la verificación de si dos imágenes son similares o en sistemas de recomendación.\n",
        "8.  Redes Neuronales Residuales (ResNet):\n",
        "\n",
        "Introducen conexiones de \"salto\" para ayudar en el entrenamiento de redes muy profundas. Han sido exitosas en la clasificación de imágenes y otras tareas.\n",
        "\n",
        "····································································································································································································································\n",
        "\n",
        "\n",
        "$$ a_{j}^{[l]} = g [ w_{j}^{[l]} · a^{[l-1]} + b_{j}^{[l]}] $$\n"
      ],
      "metadata": {
        "id": "hHRb5WA59Lg2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Primera red neuronal"
      ],
      "metadata": {
        "id": "aciovcjkOQp_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aNOJx8Bk7IFX",
        "outputId": "b6a9b6f3-9645-4385-c400-91afad8b2f19"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "M:  1787\n"
          ]
        }
      ],
      "source": [
        "import glob\n",
        "import numpy as np\n",
        "import copy\n",
        "import matplotlib.pyplot as plt\n",
        "import h5py\n",
        "import scipy\n",
        "from PIL import Image\n",
        "from scipy import ndimage\n",
        "\n",
        "%matplotlib inline\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "#Los datos corresponden a un subset del dataset cats vs dogs de https://www.kaggle.com/c/dogs-vs-cats\n",
        "files = glob.glob('/content/drive/MyDrive/ML2023/CATS_DOGS/*')\n",
        "M = len(files)\n",
        "print(\"M: \", M)\n",
        "\n",
        "#numero de pixeles para nuestras imágenes.\n",
        "num_px = 64\n",
        "\n",
        "# Entrenamiento\n",
        "\n",
        "train_x = []\n",
        "train_y = []\n",
        "for file in files[:100]:\n",
        "    img = Image.open(file)\n",
        "    img = img.resize((num_px,num_px))\n",
        "    data = np.asarray(img)\n",
        "    train_x.append(data)\n",
        "    img.close()\n",
        "    if 'dog' in file:\n",
        "        train_y.append(1)\n",
        "    if 'cat' in file:\n",
        "        train_y.append(0)\n",
        "train_y = np.asarray(train_y)\n",
        "train_x = np.asarray(train_x)\n",
        "\n",
        "# Prueba\n",
        "\n",
        "test_x = []\n",
        "test_y = []\n",
        "for file in files[1700:]:\n",
        "    #print(file)\n",
        "    img = Image.open(file)\n",
        "    img = img.resize((num_px,num_px))\n",
        "    data = np.asarray(img)\n",
        "    test_x.append(data)\n",
        "    img.close()\n",
        "    if 'dog' in file:\n",
        "        test_y.append(1)\n",
        "    if 'cat' in file:\n",
        "        test_y.append(0)\n",
        "test_y = np.asarray(test_y)\n",
        "test_x = np.asarray(test_x)\n",
        "\n",
        "train_x_flatten = train_x.reshape(train_x.shape[0], -1).T\n",
        "test_x_flatten = test_x.reshape(test_x.shape[0], -1).T\n",
        "\n",
        "train_set_y = train_y.reshape((1,train_y.shape[0]))\n",
        "test_set_y = test_y.reshape((1,test_y.shape[0]))\n",
        "\n",
        "train_set_x = train_x_flatten/255\n",
        "test_set_x = test_x_flatten/255"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Inicializar los parámetros w y b\n",
        "def initialize_parameters(dim):\n",
        "    w = np.random.uniform(0, 0.1, size=(dim, 1))\n",
        "    b = 0\n",
        "    return w, b\n",
        "\n",
        "\n",
        "# Función sigmoide\n",
        "def sigmoid(z):\n",
        "    return 1 / (1 + np.exp(-z))\n",
        "\n",
        "# Función costo\n",
        "def compute_cost(A, Y):\n",
        "    m = Y.shape[1]\n",
        "    cost = -np.sum(Y * np.log(A) + (1 - Y) * np.log(1 - A)) / m\n",
        "    return cost\n",
        "\n",
        "# Propagación hacia adelante para obtener la salida del modelo\n",
        "def forward_propagation(w, b, X):\n",
        "    Z = np.dot(w.T, X) + b\n",
        "    A = sigmoid(Z)\n",
        "    return A\n",
        "\n",
        "# Propagación hacia atrás para calcular gradientes\n",
        "def backward_propagation(X, Y, A):\n",
        "    m = X.shape[1]\n",
        "    dz = A - Y\n",
        "    dw = np.dot(X, dz.T) / m\n",
        "    db = np.sum(dz) / m\n",
        "    return dw, db\n",
        "\n",
        "# Actualizar los parámetros usando el gradiente descendente\n",
        "def update_parameters(w, b, dw, db, learning_rate):\n",
        "    w -= learning_rate * dw\n",
        "    b -= learning_rate * db\n",
        "    return w, b\n",
        "\n",
        "# Paso de un solo paso de gradiente descendente\n",
        "def gradient_descent(X, Y, w, b, learning_rate):\n",
        "\n",
        "    #Parámetros para gradiente descendente\n",
        "    Z = np.dot(w.T, X) + b\n",
        "    A = sigmoid(Z)\n",
        "    cost = compute_cost(A, Y)\n",
        "\n",
        "    dz = A - Y\n",
        "    dw = np.dot(X, dz.T) / X.shape[1]\n",
        "    db = np.sum(dz) / X.shape[1]\n",
        "\n",
        "    # Actualizamos parámetros\n",
        "    w, b = update_parameters(w, b, dw, db, learning_rate)\n",
        "    return w, b, cost\n",
        "\n",
        "# Modelo de regresión logística con gradiente descendente\n",
        "def model_with_gradient_descent(X_train, Y_train, X_test, Y_test, num_iterations, learning_rate):\n",
        "    w, b = initialize_parameters(X_train.shape[0])\n",
        "\n",
        "    # Itera para cada uno de nuestros conjuntos de entrenamiento y muestra cada 100\n",
        "    for i in range(num_iterations):\n",
        "        w, b, cost = gradient_descent(X_train, Y_train, w, b, learning_rate)\n",
        "\n",
        "        #if i % 100 == 0:\n",
        "        #    print(f\"Iteración {i}, Costo de entrenamiento: {cost}\")\n",
        "\n",
        "    return w, b"
      ],
      "metadata": {
        "id": "JznYyRn7PiAr"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Número de iteraciones\n",
        "num_iterations = 1500\n",
        "\n",
        "# Taza de aprendizaje\n",
        "learning_rate = 0.01"
      ],
      "metadata": {
        "id": "oej0A6pTPpOs"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "i = 0\n",
        "w = []\n",
        "b = []\n",
        "a = []\n",
        "\n",
        "for i in range(2):\n",
        "  # Retornamos w, a, b\n",
        "  trained_w_gd, trained_b_gd = model_with_gradient_descent(train_set_x, train_set_y, test_set_x, test_set_y, num_iterations, learning_rate)\n",
        "  predictions_gd = forward_propagation(trained_w_gd, trained_b_gd, test_set_x)\n",
        "  w.append(trained_w_gd)\n",
        "  b.append(trained_b_gd)\n",
        "  a.append(predictions_gd)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OPuRSzfPPg46",
        "outputId": "f06fb66a-d0c8-4f7c-8275-037161f40163"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-9-23ebcd8f496d>:15: RuntimeWarning: divide by zero encountered in log\n",
            "  cost = -np.sum(Y * np.log(A) + (1 - Y) * np.log(1 - A)) / m\n",
            "<ipython-input-9-23ebcd8f496d>:15: RuntimeWarning: invalid value encountered in multiply\n",
            "  cost = -np.sum(Y * np.log(A) + (1 - Y) * np.log(1 - A)) / m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "w = np.asarray(w)\n",
        "a = np.asarray(a)\n",
        "b = np.asarray(b)"
      ],
      "metadata": {
        "id": "af11wH0vQMIT"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(w.shape)\n",
        "print(a.shape)\n",
        "print(b.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cmRiIPNOSGOI",
        "outputId": "41cf01f1-cc12-4fb3-b702-b2431f67b7e3"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(2, 12288, 1)\n",
            "(2, 1, 87)\n",
            "(2,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Neurona final\n",
        "def Neurona(w, b, X):\n",
        "    A = []\n",
        "    for i in range(2):\n",
        "      Z = np.dot(w[i], X[i]) + b[i]\n",
        "    A.append(sigmoid(Z))\n",
        "    return A\n",
        "afinal = Neurona(w, b, a)\n",
        "rounded_predictions_gd = np.round(afinal)\n",
        "print(afinal)\n",
        "print(rounded_predictions_gd)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qism89kLSU8B",
        "outputId": "a0332c11-a45b-4537-c2e5-e3f85cf46d4f"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[array([[0.46040249, 0.45783245, 0.45594496, ..., 0.46044033, 0.46010038,\n",
            "        0.4560276 ],\n",
            "       [0.46041706, 0.4587822 , 0.45758118, ..., 0.46044113, 0.46022491,\n",
            "        0.45763377],\n",
            "       [0.46051712, 0.46531131, 0.46883906, ..., 0.46044662, 0.46108011,\n",
            "        0.46868449],\n",
            "       ...,\n",
            "       [0.46034871, 0.45433   , 0.4499148 , ..., 0.46043738, 0.45964083,\n",
            "        0.450108  ],\n",
            "       [0.46043664, 0.46005874, 0.45978103, ..., 0.4604422 , 0.46039223,\n",
            "        0.45979319],\n",
            "       [0.46033481, 0.4534256 , 0.44835876, ..., 0.46043662, 0.45952208,\n",
            "        0.44858045]])]\n",
            "[[[0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  ...\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]]]\n"
          ]
        }
      ]
    }
  ]
}